- hosts: storage
  tasks:
    - name: Run full upgrade
      apt:
        upgrade: full
        update_cache: true
    - name: Install htop and kitty-terminfo
      apt:
        name: 
          - htop
          - kitty-terminfo
        state: present
    # https://forums.raspberrypi.com/viewtopic.php?t=245931
    - name: Blacklist UAS for flaky SABRENT devices
      copy:
        content: "usb-storage.quirks=152d:1561:u console=tty0 console=ttyS1,115200 root=LABEL=RASPIROOT rw fsck.repair=yes net.ifnames=0 rootwait"
        path: /boot/firmware/cmdline.txt
        mode: 0644
      register: cmdline
    - name: Set hostname
      copy:
        content: "{{ inventory_hostname }}"
        dest: /etc/hostname
      register: hostname
    - name: Set hosts
      lineinfile:
        line: "127.0.1.1 {{ inventory_hostname }}"
        mode: 0644
        path: /etc/hosts
    - name: Deploy sshd config
      copy:
        src: files/sshd_config
        dest: /etc/ssh/sshd_config
        mode: 0644
      notify:
        - Restart sshd
    - name: Ensure .ssh dir exists
      file:
        path: ~/.ssh
        state: directory
        mode: 0755
    - name: Deploy authorized_keys
      copy:
        src: files/authorized_keys
        dest: ~/.ssh/authorized_keys
        mode: 0600
    - name: Check if reboot is required for package upgrade
      register: reboot_required_file
      stat:
        path: /var/run/reboot-required
    - name: Reboot host
      become: true
      become_user: root
      reboot:
        msg: Reboot initiated by Ansible
        connect_timeout: 5
        reboot_timeout: 600
        pre_reboot_delay: 0
        post_reboot_delay: 60
        test_command: uptime
      when: reboot_required_file.stat.exists or cmdline.changed or hostname.changed
    - name: Deploy SSD udev rule
      become: true
      become_user: root
      copy:
        src: files/60-ssd-scheduler.rules
        dest: /etc/udev/rules.d/60-ssd-scheduler.rules
        mode: 0644
      notify:
        - Reload udev rules
    - name: Install glusterfs
      become: true
      become_user: root
      apt:
        name: glusterfs-server
        state: present
        update_cache: true
    - name: Create LVM volume groups
      become: true
      become_user: root
      with_items:
        - /dev/sda
        - /dev/sdb
      community.general.lvg:
        pvs: "{{ item }}"
        vg: "data.{{ item | basename }}"
    - name: Create LVM thinpools
      become: true
      become_user: root
      with_items:
        - data.sda
        - data.sdb
      community.general.lvol:
        thinpool: "{{ item }}-pool"
        vg: "{{ item }}"
    - name: Create LVM bricks
      become: true
      become_user: root
      with_items:
        - data.sda
        - data.sdb
      community.general.lvol:
        lv: brick
        size: 450G
        thinpool: "{{ item }}-pool"
        vg: "{{ item }}"
    - name: Set up XFS on LVM bricks
      become: true
      become_user: root
      with_items:
        - /dev/data.sda/brick
        - /dev/data.sdb/brick
      community.general.filesystem:
        dev: "{{ item }}"
        fstype: xfs
    - name: Create mount points for LVM bricks
      become: true
      become_user: root
      with_items:
        - /data/sda/brick
        - /data/sdb/brick
      file:
        path: "{{ item }}"
        mode: 0755
        state: directory
    - name: Set up fstab rules for LVM bricks
      become: true
      become_user: root
      with_items:
        - sda
        - sdb
      lineinfile:
        path: /etc/fstab
        line: "/dev/data.{{ item }}/brick /data/{{ item }}/brick xfs defaults 0 0"
    - name: Mount LVM bricks
      become: true
      become_user: root
      command:
        cmd: mount -a
    - name: Enable & Start GlusterFS server
      become: true
      become_user: root
      systemd:
        name: glusterd
        enabled: true
        state: started
  handlers:
    - name: Reload udev rules
      become: true
      become_user: root
      command:
        cmd: udevadm control --reload-rules
    - name: Restart sshd
      systemd:
        name: sshd.service
        enabled: true
        state: restarted
